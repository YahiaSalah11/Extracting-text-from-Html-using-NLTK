{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_html_text(url):\n",
    "    try:\n",
    "        # Fetch HTML content from the URL\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Raise an exception for any HTTP error\n",
    "        html_content = response.text\n",
    "        \n",
    "        # Parse HTML using BeautifulSoup\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "        \n",
    "        # Extract text from the parsed HTML\n",
    "        text = soup.get_text()\n",
    "        \n",
    "        return text\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(\"Error fetching URL:\", e)\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    tokens = []\n",
    "    current_token = \"\"\n",
    "    for char in text:\n",
    "        if char.isalnum():  # Check if the character is alphanumeric\n",
    "            current_token += char\n",
    "        else:\n",
    "            if current_token:\n",
    "                tokens.append(current_token.lower())  # Add the token to the list\n",
    "                current_token = \"\"\n",
    "            if char.strip():  # Check if the character is not whitespace\n",
    "                tokens.append(char)  # Add non-alphanumeric characters as separate tokens\n",
    "    if current_token:\n",
    "        tokens.append(current_token.lower())  # Add the last token if any\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Remove non-alphanumeric characters and convert to lowercase\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text).lower()\n",
    "\n",
    "    \n",
    "    # Tokenize the text\n",
    "    tokens = tokenize(text)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
    "    \n",
    "    # Stemming\n",
    "    stemmer = PorterStemmer()\n",
    "    stemmed_tokens = [stemmer.stem(word) for word in filtered_tokens]\n",
    "    \n",
    "    return stemmed_tokens\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "url = \"https://edusites.uregina.ca/brandymogg/2021/06/11/i-am-a-baker-but-please-dont-ask-me-to-bake-a-cake-in-code/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique words: {'final', 'right', 'acknowledg', 'pennington', 'troubl', 'quotat', 'reward', 'dr', 'doesnt', 'squar', 'resourc', 'haili', 'manifesto', 'everyon', 'sort', 'video', 'next', 'figur', 'replyyour', 'back', 'imagin', 'day', 'realli', 'someth', 'first', 'recent', 'java', 'skill', 'last', 'jun', 'physic', 'knowledg', 'nice', 'entri', 'orient', 'done', 'blog', 'krengnektak', 'basic', 'wordpressorg', 'believ', 'ago', 'june', 'get', 'gone', 'critic', 'far', 'second', 'though', 'search', 'use', 'real', 'see', 'futur', 'websit', 'error', 'hit', 'certain', 'import', 'got', 'twitter', 'class', 'teacher', 'come', 'project', 'tri', 'anyon', 'make', 'stick', 'coveragenext', 'sinc', 'pretti', 'bribe', 'e', 'given', 'put', 'literaci', 'skip', 'dobak', 'watch', 'job', 'grade', 'ever', 'categori', 'us', 'rule', 'develop', 'game', 'insert', 'attempt', 'embarrassingli', 'amaz', 'khavari', 'give', 'notion', 'mogg', 'cleanportfolio', 'avail', 'includ', 'endin', 'share', 'life', 'especi', 'either', 'lesson', 'like', 'social', 'follow', 'particular', 'brave', 'home', 'highli', 'browser', 'love', 'etc', 'digit', 'long', 'bite', 'baker', 'children', 'hous', 'look', 'take', 'brandi', 'point', 'problemsolv', 'uncategor', 'least', 'hour', 'feel', 'instead', 'activ', 'commentstina', 'nothaili', 'field', 'difficult', 'dolcini', 'would', 'copyright', 'mark', 'name', 'need', 'road', 'cheesecak', 'math', 'edti', 'becom', 'found', 'els', 'say', 'big', 'speak', 'overli', 'head', 'approach', 'commend', 'process', 'much', 'news', 'convent', 'html', 'post', 'week', 'year', 'yup', 'enjoy', 'weekstaylor', 'cake', 'dont', 'part', 'mention', 'scienc', 'email', 'weeksrosali', 'learnt', 'wrong', 'alot', 'your', 'may', 'homemad', 'novemb', 'insight', 'requir', 'spent', 'pick', 'gabbi', 'tokor', 'understand', 'end', 'excel', 'half', 'address', 'code', 'thank', 'teach', 'help', 'learn', 'content', 'way', 'detail', 'decid', 'continu', 'catch', 'educ', 'opportun', 'write', 'rememb', 'experi', 'decemb', 'cancel', 'log', 'gledi', 'essenti', 'think', 'rather', 'start', 'health', 'postfrom', 'conclus', 'block', 'english', 'step', 'struggl', 'blm', 'moment', 'never', 'level', 'evolv', 'order', 'cinnamon', 'toci', 'well', 'curriculum', 'blake', 'student', 'feed', 'best', 'pleas', 'reserv', 'hard', 'portfolio', 'dash', 'plan', 'thing', 'one', 'publish', 'frustrat', 'complet', 'theme', 'offer', 'agre', 'empow', 'menu', 'someon', 'differenti', 'word', 'edusit', 'correct', 'great', 'ctv', 'bake', 'chose', 'know', 'base', 'work', 'save', 'refer', 'meta', 'studi', 'technolog', 'ask', 'hilli', 'bun', 'time', 'expos', 'drjava', 'mani', 'extrem', 'coffe', 'want', 'world', 'find', 'space', 'leav', 'jump', 'profession', 'edt', 'tik', 'repli', 'zero', 'anoth', 'serious', 'navig', 'previou', 'trip', 'actual', 'instruct', 'ill', 'hope', 'motiv', 'scroll', 'size', 'languag', 'begin', 'comment', 'archiv', 'howev', 'even', 'tell'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "html_text = get_html_text(url)\n",
    "if html_text:\n",
    "    preprocessed_tokens = preprocess_text(html_text)\n",
    "    unique_words = set(preprocessed_tokens)\n",
    "    print(\"Unique words:\", unique_words)\n",
    "else:\n",
    "    print(\"Failed to fetch or parse HTML content.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dr\n",
      "e\n",
      "us\n"
     ]
    }
   ],
   "source": [
    "for word in unique_words:\n",
    "    if len(word) < 3 :\n",
    "        print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
